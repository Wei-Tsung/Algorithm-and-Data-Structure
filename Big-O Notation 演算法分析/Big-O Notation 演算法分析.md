




## 為什麼演算法的分析很重要 ?

<img src='https://github.com/Wei-Tsung/Core-Concepts-Visualization/blob/master/big%20o%20notation%20%E6%BC%94%E7%AE%97%E6%B3%95%20%E5%89%AA%E8%A3%81.png' width= 800 height= 600>


<img src='https://github.com/Wei-Tsung/Core-Concepts-Visualization/blob/master/BigOTime.png' width = 800>

### Introduction 概述 :

- 當數據量尺度還小的時候 , 演算法執行效率的優劣通常會被人給忽略

因為同樣都能夠執行 , 在尺度小的情況下 , 一般人在感受上不太明顯

但是隨著數據規模逐漸開始變大 , 演算法的優劣就會直接決定一個專案和企業的成本差距  , 而差距會隨著規模迅速擴大
這時演算法優劣的分析和比較 , 就變得非常地重要 ;


舉例來說 : 有個人去一家便利商店繳費 , 其中一家收了手續費10 元 , 另一家較遠的超商卻只收1元  
一般人可能會覺得 , 反正前面收10元的那一家比較近 , 也沒差多少元 , 
實在不必自找麻煩只為了省幾塊錢又大費周章跑到較遠的超商去繳費了 ;

但是如果你仔細的觀察 , 兩者費率的差距其實是高達了10倍 

如果今天把量的尺度放大 , 一家企業的成本是100萬  , 
另一家企業的成本卻是1000萬 , 你是否還會覺得這件事情無關緊要呢?


### Time Complextiy
---

當分析演算法的優劣時 ,
一般人的第一個想法通常是會想要直接比較演算法所耗費時間的多寡 , 來評價演算法的優劣 ;

但是因為每一個演算法和軟體執行時所處在的平台和硬體環境皆不相同 ,
因此直接比較演算法執行時所耗費時間的多寡 , 並不是一個客觀的比較標準

- 所以評估演算法優劣的分析時 : 會採用演算法執行時  所需耗費時間/成本 增加的速度來做為較為客觀的衡量標準 -



在演算法中，常會使用 Big O Notation 和 Time Complextiy 來衡量一個演算法的好壞
Big-Ο Notation 代表演算法時間函式的上限(Upper bound)
在最壞的狀況下，演算法的執行時間不會超過Big-Ο
> 定義：

>> 格式：f(n) = Ο(g(n)) iff ∃正常數c和n0，使得f(n) ≤ c × g(n), ∀ n >= n0

>> n：輸入資料大小
>> f(n)：理想狀況下，程式在電腦中實際執行指令次數
>> g(n)：執行時間的成長率
>> 若輸入資料量(n)比(n0)多時，則時間函數f(n)必會小於等於g(n)
>> 當輸入資料量大到一定程度時，則c×g(n)必定會大於實際執行指令次數
>> cg(n)相當於f(n)的上限，在最壞情況下(Worst Case)，f(n)的成長率最多到g(n)，而不會超過它
